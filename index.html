<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ultrasound Report Generation DPO </title>
    <!-- MathJax for LaTeX rendering -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Ultrasound Report Generation DPO </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Nikkhah, Ali</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Heidari, Moein</a><sup>*</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>














<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
  <!-- ===== HERO ===== -->
  <section class="hero is-primary is-bold">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h1 class="title is-2">Ultrasound Report Generation with DPO</h1>
        <h2 class="subtitle is-4">&amp; Survey of Open-Source Biomedical Language Models</h2>
      </div>
    </div>
  </section>
      <section>
        <h1>Ultrasound Entity Recognition</h1>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Medical-NER Findings & Triplet Extraction Flow</h2>
    <div class="content">
      <h3 class="title is-4">NER Model Performance - BioBERT & Family</h3>
      <p><strong>BioBERT</strong> (and related models like BlueBERT/ClinicalBERT) outperform vanilla BERT and SciBERT by a margin, due to their pre-training on PubMed and clinical notes. This enables better recognition of domain-specific phrases and negations.</p>
      <p>Reported F1 improvements (2-6 percentage points over BERT/SciBERT) are primarily from better recall of edge-case entities and fewer false positives.</p>

      <h3 class="title is-4">Implications for Triplet Extraction</h3>
      <p>A strong NER stage is crucial since any errors in entity recognition or hallucinated entities will propagate to the triplet extraction stage. Using a BioBERT-style model as the backbone, especially fine-tuned on ultrasound reports, will result in cleaner <code>{entity, position, exist}</code> triples.</p>

      <h4 class="subtitle is-5">Negation and Uncertainty Detection</h4>
      <p>Tools like <strong>NegBio</strong> (which employs dependency-pattern rules) can slot in after NER to determine the "existence" flag, improving the triplet consistency.</p>

      <h4 class="subtitle is-5">Position Phrase Parsing</h4>
      <p>Ultrasound-specific positions (e.g., "sub-hepatic region", "right adnexa") require curated lexicons or relation parsers like OSCAR/NegBio to map phrases to standard anatomical locations.</p>

      <h3 class="title is-4">Ultrasound-Specific Considerations</h3>
      <ul>
        <li>Ultrasound reports are typically shorter and feature more shorthand abbreviations (e.g., "Rt ov nl", "CBD 7 mm"). Thus, domain-adapted tokenization and abbreviation mapping are essential during preprocessing.</li>
        <li>Many findings come with size measurements, which can either be handled as separate entity tokens (e.g., "cyst") plus an attribute triple (e.g., "size, 3 cm, true") or embedded within the entity span depending on the downstream task.</li>
      </ul>

      <h3 class="title is-4">End-to-End NER + Triplet Flow</h3>
      <p>Below is the revised pipeline for processing ultrasound reports:</p>
      <ol>
        <li><strong>Pre-process & Tokenize:</strong> Expand abbreviations, keep measurement tokens.</li>
        <li><strong>NER with BioBERT-family:</strong> Fine-tune BioBERT on manually-annotated ultrasound sentences for entity recognition.</li>
        <li><strong>Negation Detection:</strong> Use NegBio (or similar rule-based methods) to set the "exist" flag (True/False/Unknown).</li>
        <li><strong>Position Parsing:</strong> Use RadGraph or rule-based models for anatomical phrase matching (e.g., "left ovary").</li>
        <li><strong>Triplet Assembly:</strong> Assemble <code>{entity, position, exist}</code> triples.</li>
        <li><strong>Post-filtering:</strong> Optionally filter inconsistent triples (e.g., remove entities with "exist = False" or empty positions).</li>
      </ol>

      <h3 class="title is-4">Entity Translation & MedKLIP Integration</h3>
      <p>Incorporating an <strong>Entity Translation</strong> layer, inspired by MedKLIP’s approach, will enhance the robustness and generalization of the NER system. Here’s how:</p>

      <ol>
        <li><strong>Canonical Concept Lookup:</strong> Map each raw entity to a canonical concept (e.g., "cyst" → UMLS CUI) using internal lookups or APIs.</li>
        <li><strong>Definition Sentence Retrieval:</strong> Use knowledge sources (e.g., Wikipedia, Radiopaedia) to fetch definitions for each entity.</li>
        <li><strong>Text-Encode Definitions:</strong> Encode these definitions with a model like ClinicalBERT to obtain embeddings, which carry richer semantic meaning.</li>
        <li><strong>Cache Vectors:</strong> Cache the resulting entity vectors for speed and continual updates.</li>
      </ol>

      <h3 class="title is-4">Final Triplet Assembly</h3>
      <p>The final output format of each triplet includes:</p>
      <pre>
        {
          "entity": {
            "text": "cyst",
            "cui": "C0010692",
            "embedding": [ … ]
          },
          "position": {
            "label": "left ovary",
            "prompt_embedding": [ … ]
          },
          "exist": "TRUE"
        }
      </pre>

      <h3 class="title is-4">Practical Roadmap</h3>
      <p>The following milestones outline the implementation effort:</p>
      <ul>
        <li><strong>MVP:</strong> NER + NegBio + lookup table of ~150 ultrasound entities and definitions (1-2 weeks)</li>
        <li><strong>Fine-tuning:</strong> Fine-tune BioBERT on ~500 annotated ultrasound sentences (2-3 days GPU)</li>
        <li><strong>CUI Linker:</strong> Build CUI linker with QuickUMLS + rules (1 week)</li>
        <li><strong>Optional:</strong> Train relation-aware model (PURE or RadGraph) for position dependencies (later)</li>
      </ul>

      <h3 class="title is-4">Key Takeaways</h3>
      <ul>
        <li>Maintain the solid NER/negation backbone.</li>
        <li>Insert a lightweight entity-translation module that turns each span into a knowledge-rich definition vector.</li>
        <li>The richer embeddings will improve synonym collapse, zero-shot robustness, and multi-modal adaptability.</li>
      </ul>

      <p>Let me know which part you'd like to explore next—definition harvesting, CUI mapping, or embedding architecture!</p>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">1️⃣ Quick Tour of Annotation Options</h2>
    <div class="content">
      <p>When building a Named Entity Recognition (NER) system, choosing the right annotation tool is crucial for both efficiency and accuracy. Here are some top options:</p>
      
      <table class="table is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Tool</th>
            <th>What it is</th>
            <th>Stand-out Strengths</th>
            <th>Things to Know / Limits</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Label Studio (open-source, MIT)</td>
            <td>A browser-based annotation server for sequence-labeling, span-labeling, bounding-box, audio, etc.</td>
            <td>
              <ul>
                <li>Free community edition</li>
                <li>Team-friendly: add users, track contributions</li>
                <li>Rich import/export options (JSON, CSV, COCO, spaCy, BIO)</li>
                <li>Plug-in system for model pre-annotations</li>
              </ul>
            </td>
            <td>
              <ul>
                <li>Requires hosting a Postgres database and web server</li>
                <li>No built-in active learning; needs custom setup</li>
              </ul>
            </td>
          </tr>
          <tr>
            <td>Prodigy (commercial, by spaCy team)</td>
            <td>A lightweight, scriptable webapp launched via a Python script for NER annotation.</td>
            <td>
              <ul>
                <li>Highly scriptable; every action is a Python one-liner</li>
                <li>Tight integration with spaCy</li>
                <li>Active-learning queues to focus on uncertain examples</li>
              </ul>
            </td>
            <td>
              <ul>
                <li>Paid license (~$390 per seat, no SaaS)</li>
                <li>One user per license; requires shared DB for multi-annotator work</li>
              </ul>
            </td>
          </tr>
          <tr>
            <td>Plain JSON / Text Editor</td>
            <td>Use any text editor (VS Code, Sublime, Excel) to manually edit exported NER labels.</td>
            <td>
              <ul>
                <li>Zero setup, 100% free</li>
                <li>Good for correcting small sets (50–100 mistakes)</li>
              </ul>
            </td>
            <td>
              <ul>
                <li>Tedious and error-prone for long sentences</li>
                <li>No sentence context visible, so harder to verify spans</li>
              </ul>
            </td>
          </tr>
        </tbody>
      </table>

      <h4 class="subtitle is-5">Recommended Path for Ultrasound Reports</h4>
      <p>The following steps are recommended for effectively annotating ultrasound reports:</p>
      <ol>
        <li>Generate weak labels automatically using <code>pseudo_label.py</code>.</li>
        <li>Import the labels into Label Studio using the "Sequence Labeling" template and BIO format.</li>
        <li>Have a radiologist (or yourself) correct ~300 sentences to clean the model’s worst mistakes.</li>
        <li>Export the corrected data to spaCy or BIO format and fine-tune the NER model for one epoch.</li>
        <li>(Optional) Switch to Prodigy for active learning if you want a continuous feedback loop.</li>
      </ol>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Domain-Adaptive Pretraining (MLM before NER)</h2>
    <div class="content">
      <p>Large-scale NLP systems like BERT use domain-adaptive pretraining (DAPT) to better capture the language of specific domains like ultrasound reports. Here’s the process:</p>
      
      <h3 class="subtitle is-4">DAPT / MLM (Masked Language Modeling)</h3>
      <p>Pre-train BERT using raw ultrasound report text, teaching the model the language of ultrasound (e.g., abbreviations, anatomical terms, measurement patterns) without requiring labels.</p>
      <p><strong>Inputs:</strong> Raw report text.</p>
      <p><strong>What you train:</strong> A masked-language-model head (same as original BERT). The whole network is optimized (or LoRA adapters are used if VRAM-limited).</p>
      <p><strong>Cost (time/VRAM):</strong> Light. With LoRA and 8-bit precision, a 3070 4GB GPU can complete 3 epochs over ~50k sentences in under 1 hour.</p>

      <h3 class="subtitle is-4">(Optional) TAPT (Task-Adaptive Pretraining)</h3>
      <p>TAPT involves a second quick pass of pretraining using only the text that will appear in your labelled split. This can improve final NER results if labels are very small.</p>
      <p><strong>Inputs:</strong> Sentences you plan to label.</p>
      <p><strong>Cost:</strong> Minimal (minutes).</p>

      <h3 class="subtitle is-4">Fine-tune NER</h3>
      <p>Fine-tune the BERT model to output BIO tags based on the weak labels, which can be corrected by annotators.</p>
      <p><strong>Inputs:</strong> Weak + corrected labels (even just 1-2k sentences is enough).</p>
      <p><strong>What you train:</strong> The classifier head and a subset of encoder weights (LoRA can be used here for efficiency).</p>
      <p><strong>Cost:</strong> Fast (minutes on a laptop).</p>
    </div>
  </div>
</section>

        <h2>1. Core Ontology and Medical Databases</h2>
        <table border="1" cellpadding="5">
            <thead>
                <tr>
                    <th>Tier</th>
                    <th>Resource / API</th>
                    <th>What it gives you (Entity, Location, Synonyms)</th>
                    <th>How to integrate</th>
                    <th>Notes & Links</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Core Ontology</strong></td>
                    <td><strong>UMLS Metathesaurus</strong></td>
                    <td>Over 4 million CUIs covering disorders, findings, anatomy, devices, procedures. Synonym lists included.</td>
                    <td>1. Download yearly release <br> 2. Filter by semantic types (e.g., T033 – “Finding”, T023 – “Body part”) <br> 3. Generate JSON gazetteer for entity auto-completion.</td>
                    <td>“One-stop shop” for comprehensive medical terms. Filter aggressively. <br> <a href="https://www.nlm.nih.gov/research/umls/Snomed/SNOMED_CT_User_Guide_20080731.pdf?utm_source=chatgpt.com">SNOMED CT User Guide</a></td>
                </tr>
                <tr>
                    <td></td>
                    <td><strong>SNOMED CT</strong></td>
                    <td>Gold-standard hierarchy for clinical findings with explicit anatomical site relations.</td>
                    <td>Normalize location phrases like "RUQ", "right liver lobe", "segment VI" using the same concept ID.</td>
                    <td>Public release via NLM license. <a href="https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon?utm_source=chatgpt.com">RadLex</a></td>
                </tr>
                <tr>
                    <td></td>
                    <td><strong>RadLex</strong></td>
                    <td>Imaging-specific lexicon (lesions, artifacts, views, ultrasound probes, planes).</td>
                    <td>Append to UMLS for disambiguating imaging jargon.</td>
                    <td>Free from RSNA. <a href="https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon?utm_source=chatgpt.com">RadLex radiology lexicon</a></td>
                </tr>
                <tr>
                    <td><strong>Light-weight, use-from-Python</strong></td>
                    <td><strong>QuickUMLS / medSpaCy-QuickUMLS</strong></td>
                    <td>Fast, fuzzy string-to-UMLS matcher returning [CUI, term, similarity].</td>
                    <td>Use as post-processor to match LLM proposals to UMLS terms with similarity above a set threshold.</td>
                    <td>Pip-installable, no license hurdles. <a href="https://github.com/Georgetown-IR-Lab/QuickUMLS?utm_source=chatgpt.com">QuickUMLS GitHub</a></td>
                </tr>
                <tr>
                    <td><strong>Structured location schema (ultrasound-friendly)</strong></td>
                    <td><strong>FMA (Foundational Model of Anatomy)</strong></td>
                    <td>Parent/child relations (e.g., "liver → lobe → segment"). Ideal for hierarchical location labels.</td>
                    <td>Use a three-level subset (organ, lobe/quadrant, sub-region) for location labels.</td>
                    <td></td>
                </tr>
                <tr>
                    <td><strong>Specialty Add-ons</strong></td>
                    <td><strong>HPO (phenotypes for obstetric US), LOINC (panels for vascular flow studies)</strong></td>
                    <td>Use if covering subspecialties like obstetric ultrasound or vascular studies.</td>
                    <td></td>
                    <td></td>
                </tr>
            </tbody>
        </table>
    </section>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Specialized Resources for Ultrasound Report Annotation</h2>
    <div class="content">
      <table class="table is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Vocabulary</th>
            <th>File(s) You Need</th>
            <th>Direct Download Link</th>
            <th>License Note</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>RadLex 4.2</strong> (radiology observations + anatomy)</td>
            <td>RadLex_OWL4.2.zip (contains radlex.owl)</td>
            <td><a href="https://radlex.org/download/RadLex_OWL4.2.zip" target="_blank">RadLex 4.2 Download</a></td>
            <td>Free for research (RSNA license prompt)</td>
          </tr>
          <tr>
            <td><strong>ICD-10-CM FY-2025</strong> (disease codes)</td>
            <td>Code-descriptions-April-2025.zip – CSV of ICD-10-CM codes & text <br> icd10cm-table-index-April-2025.zip – Alphabetical index</td>
            <td><a href="https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD10CM/2025-Update/Code-desciptions-April-2025.zip" target="_blank">ICD-10-CM FY 2025 Codes</a> <br> <a href="https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD10CM/2025-Update/icd10cm-table-index-April-2025.zip" target="_blank">ICD-10-CM Index</a></td>
            <td>Public-domain</td>
          </tr>
          <tr>
            <td><strong>SNOMED CT INT 2024-07</strong> (findings, qualifiers)</td>
            <td>SnomedCT_InternationalRF2_production.zip</td>
            <td><a href="https://www.nlm.nih.gov/research/umls/Snomed/SNOMED_CT_User_Guide_20080731.pdf" target="_blank">SNOMED CT Overview</a></td>
            <td>Non-commercial research allowed</td>
          </tr>
          <tr>
            <td><strong>MIDO</strong> (medical-imaging ontology, ultrasound terms)</td>
            <td>mido.owl via BioPortal</td>
            <td><a href="https://bioportal.bioontology.org/ontologies/MIDO?download_file=true" target="_blank">MIDO Download</a></td>
            <td>CC-BY</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subtitle is-4">Other Resources</h3>
      <ul>
        <li><strong>BI-RADS (Breast Imaging-Reporting and Data System):</strong> A standardized system for reporting breast imaging findings, including those from ultrasound. Available from the American College of Radiology.</li>
        <li><strong>LI-RADS (Liver Imaging Reporting and Data System):</strong> A system for classifying liver lesions based on imaging features, including those observed in ultrasound. Guidelines are publicly available from the American College of Radiology.</li>
        <li><strong>Medical Imaging and Diagnostic Ontology (MIDO):</strong> Available on BioPortal, designed to standardize data in medical imaging tasks, including disease classification and imaging modalities.</li>
      </ul>

      <h3 class="subtitle is-4">Tools for Integrating These Resources</h3>
      <ul>
        <li><a href="https://github.com/med-spacy/medspaCy" target="_blank"><strong>medSpaCy</strong></a>: A SpaCy extension for clinical text processing, including pre-trained models for NER in medical contexts.</li>
        <li><a href="https://github.com/allenai/scispacy" target="_blank"><strong>scispaCy</strong></a>: A SpaCy-based library providing pre-trained models for scientific and clinical text processing.</li>
        <li><a href="https://www.johnsnowlabs.com/spark-nlp/" target="_blank"><strong>Spark NLP for Healthcare</strong></a>: A library offering pre-trained models for healthcare NLP tasks, including NER and clinical entity resolution.</li>
      </ul>
    </div>
  </div>
</section>

    <section>
        <h2>2. Hybrid Approaches and Pre-trained Models</h2>
        <table border="1" cellpadding="5">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Training Corpus</th>
                    <th>Open-Source?</th>
                    <th>Params (M)</th>
                    <th>PubMedQA Acc. (%)</th>
                    <th>BC5-Disease F1 (%)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>BioBERT</td>
                    <td>PubMed + PMC abstracts</td>
                    <td class="has-text-success">✔</td>
                    <td>110</td>
                    <td>60.2</td>
                    <td>84.7</td>
                </tr>
                <tr>
                    <td>SciBERT</td>
                    <td>Semantic Scholar</td>
                    <td class="has-text-success">✔</td>
                    <td>110</td>
                    <td>57.4</td>
                    <td>84.5</td>
                </tr>
                <tr>
                    <td>BlueBERT</td>
                    <td>PubMed + MIMIC-III</td>
                    <td class="has-text-success">✔</td>
                    <td>110</td>
                    <td>49.1</td>
                    <td>83.0</td>
                </tr>
                <tr>
                    <td>BioGPT-Large</td>
                    <td>PubMed abstracts</td>
                    <td class="has-text-success">✔</td>
                    <td>1500</td>
                    <td><strong>78.2</strong></td>
                    <td>45.0†</td>
                </tr>
                <tr>
                    <td>PMC LLaMA-13B</td>
                    <td>PMC full-text</td>
                    <td class="has-text-success">✔</td>
                    <td>13000</td>
                    <td>76.8</td>
                    <td>—</td>
                </tr>
            </tbody>
        </table>
    </section>


  <!-- ===== MODEL TABLE SECTION ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Top Performing Open-Source Biomedical LMs</h2>
    <div class="content">
      <table class="table is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Model</th>
            <th>Training Corpus</th>
            <th>Open-Source?</th>
            <th>Params (M)</th>
            <th>PubMedQA Acc. (%)</th>
            <th>BC5-Disease F1 (%)</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>BioBERT</td><td>PubMed + PMC abstracts</td><td class="has-text-success">✔</td><td>110</td><td>60.2</td><td>84.7</td></tr>
          <tr><td>SciBERT</td><td>Semantic Scholar</td><td class="has-text-success">✔</td><td>110</td><td>57.4</td><td>84.5</td></tr>
          <tr><td>BlueBERT</td><td>PubMed + MIMIC-III</td><td class="has-text-success">✔</td><td>110</td><td>49.1</td><td>83.0</td></tr>
          <tr><td>PubMedBERT</td><td>PubMed abstracts/full-text</td><td class="has-text-success">✔</td><td>110</td><td>55.8</td><td>85.6</td></tr>
          <tr><td>BioGPT-Large</td><td>PubMed abstracts</td><td class="has-text-success">✔</td><td>1500</td><td><strong>78.2</strong></td><td>45.0†</td></tr>
          <tr><td>BioMedLM</td><td>PubMed + PMC full-text</td><td class="has-text-success">✔</td><td>2700</td><td>74.4</td><td>—</td></tr>
          <tr><td>PMC LLaMA-13B</td><td>PMC full-text</td><td class="has-text-success">✔</td><td>13000</td><td>76.8</td><td>—</td></tr>
          <tr><td><strong>BioMegatron-345M</strong></td><td>PubMed + clinical notes</td><td class="has-text-success">✔</td><td>345</td><td>—</td><td>—</td></tr>
          <tr><td><strong>BioMed-RoBERTa-large (355M)</strong></td><td>Biomedical corpus (PubMed, etc.)</td><td class="has-text-success">✔</td><td>355</td><td>—</td><td>—</td></tr>
          <tr><td><strong>BioGPT-Large (1.5B)</strong></td><td>PubMed abstracts</td><td class="has-text-success">✔</td><td>1500</td><td>—</td><td>—</td></tr>
          <tr><td><strong>Llama-2-7B (general-domain)</strong></td><td>General domain + PubMed continued pretrain</td><td class="has-text-success">✔</td><td>7000</td><td>—</td><td>—</td></tr>
        </tbody>
      </table>
      <p class="is-size-7">† BioGPT numbers are zero-shot; all others are fine-tuned on BLURB.</p>
    </div>
  </div>
</section>

  <section>
    <h2>3. Comparison of NLP Methods for Ultrasound Entity Recognition</h2>
    <p>In the process of extracting and tagging entities, locations, and existence from ultrasound reports, various NLP methods can be employed. These methods can be broadly categorized into classic NLP, medical NLP, ontology-based approaches, and transformer-based models. Each method comes with its strengths and applications depending on the complexity and specificity of the task.</p>
    
    <h3>Classic NLP Methods</h3>
    <p>Classic NLP methods focus on general-purpose text processing tasks such as tokenization, part-of-speech tagging, and named entity recognition. These methods can be adapted for medical texts, but they may require additional customization and domain-specific tuning for medical applications.</p>
    
    <table border="1" cellpadding="5">
        <thead>
            <tr>
                <th>Method</th>
                <th>Description</th>
                <th>Pros</th>
                <th>Cons</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>spaCy (Classic NLP)</strong></td>
                <td>A robust NLP library for general-purpose tasks like tokenization, POS tagging, and NER.</td>
                <td>Fast, well-documented, customizable with pre-trained models.</td>
                <td>May lack domain-specific training, requiring fine-tuning for medical data.</td>
                <td>General entity recognition from ultrasound reports, but with limited medical accuracy.</td>
            </tr>
            <tr>
                <td><strong>NLTK (Classic NLP)</strong></td>
                <td>A toolkit for NLP tasks, including tokenization, POS tagging, and entity extraction.</td>
                <td>Widely used, offers flexibility in preprocessing and experimentation.</td>
                <td>May not perform as efficiently on large datasets; lacks pre-trained domain models.</td>
                <td>Entity extraction when domain-specific models are unavailable.</td>
            </tr>
        </tbody>
    </table>

    <h3>Medical NLP Methods</h3>
    <p>Medical NLP models, like MedSpaCy, are designed to work specifically with medical texts and provide improved performance for tasks like extracting medical entities and understanding medical terminology.</p>

    <table border="1" cellpadding="5">
        <thead>
            <tr>
                <th>Method</th>
                <th>Description</th>
                <th>Pros</th>
                <th>Cons</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>MedSpaCy (Medical NLP)</strong></td>
                <td>An extension of spaCy optimized for medical entities, offering pre-trained models for clinical NER.</td>
                <td>Well-suited for medical text, can identify disease, anatomical locations, and clinical terms.</td>
                <td>Requires specialized setup and dependency management.</td>
                <td>Clinical reports, including ultrasound, where domain-specific terminology is used.</td>
            </tr>
        </tbody>
    </table>

    <h3>Ontology-based Methods</h3>
    <p>Ontology-based approaches leverage medical ontologies, such as UMLS, SNOMED CT, and RadLex, to map extracted entities to standardized medical concepts. These methods ensure consistency and precision in medical terminology, which is especially useful when dealing with complex medical texts.</p>

    <table border="1" cellpadding="5">
        <thead>
            <tr>
                <th>Method</th>
                <th>Description</th>
                <th>Pros</th>
                <th>Cons</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>UMLS (Unified Medical Language System)</strong></td>
                <td>A comprehensive system that links biomedical vocabularies and provides a unified view of medical concepts.</td>
                <td>Extensive coverage of medical terms and relationships.</td>
                <td>Large and complex; requires API access and data filtering for specific tasks.</td>
                <td>Standardizing medical terminology and ensuring consistency across ultrasound data.</td>
            </tr>
            <tr>
                <td><strong>SNOMED CT</strong></td>
                <td>A global standard for clinical terms and relationships, widely used for medical coding and clinical data exchange.</td>
                <td>Provides a structured hierarchy for entities and relationships.</td>
                <td>Can be complex to implement and requires deep integration into systems.</td>
                <td>Mapping locations and clinical findings in ultrasound reports.</td>
            </tr>
            <tr>
                <td><strong>RadLex</strong></td>
                <td>A lexicon specifically designed for radiology, offering standardized terms for imaging findings, including ultrasound-specific probes and views.</td>
                <td>Well-suited for imaging-specific data and terminology.</td>
                <td>Limited scope compared to broader ontologies like UMLS.</td>
                <td>Mapping ultrasound imaging terms and findings (e.g., "cyst," "lesion").</td>
            </tr>
        </tbody>
    </table>

    <h3>BERT-based Approaches</h3>
    <p>Transformer-based models like BERT and its variants (BioBERT, ClinicalBERT) have revolutionized the field of medical NLP by offering pre-trained models that excel at understanding the complex semantics of medical texts, including ultrasound reports.</p>

    <table border="1" cellpadding="5">
        <thead>
            <tr>
                <th>Method</th>
                <th>Description</th>
                <th>Pros</th>
                <th>Cons</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>BioBERT</strong></td>
                <td>A pre-trained BERT model for biomedical text mining, trained on large biomedical corpora like PubMed.</td>
                <td>Excellent for biomedical entity recognition and understanding domain-specific language.</td>
                <td>Requires fine-tuning for specific tasks, computationally intensive.</td>
                <td>Advanced entity recognition in ultrasound reports and medical diagnoses.</td>
            </tr>
            <tr>
                <td><strong>ClinicalBERT</strong></td>
                <td>A fine-tuned version of BERT, pre-trained on clinical text data, ideal for clinical NER tasks.</td>
                <td>Great for clinical documents, including ultrasound reports.</td>
                <td>May not perform as well on non-clinical data.</td>
                <td>Extraction of entities like diseases and treatments from ultrasound reports.</td>
            </tr>
        </tbody>
    </table>

    <h3>Saving the Tagged Data</h3>
    <p>After the entities have been extracted using any of the above methods, the tagged data can be saved in a variety of formats, such as CSV, JSON, or a database, for further analysis, training, or integration into larger workflows.</p>

    <h3>Comparison of Tagging Results Across Different Methods</h3>
    <p>The effectiveness of these methods can be compared using evaluation metrics like precision, recall, and F1-score. Some models like BioBERT and MedSpaCy, trained specifically on medical data, tend to outperform general-purpose models like spaCy in extracting medical entities.</p>
</section>
          <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">🔍 Unsupervised Learning for Ultrasound Report Annotation</h2>
    <div class="content">
      <p>Unsupervised learning methods are crucial for handling *untagged* ultrasound reports where no labeled data is available. These techniques allow you to detect entities, their types, and handle negation without requiring manual annotations. Here are the most relevant methods:</p>

      <table class="table is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Method</th>
            <th>Core Idea</th>
            <th>Why It Matters for <em>Untagged</em> Ultrasound Reports</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Unsupervised BioNER Framework (Bian et al., 2023)</strong></td>
            <td>Treats NER as two unsupervised steps: (1) entity-span discovery using LLM prompts, (2) entity-type assignment by retrieving similar concepts from UMLS and letting the LLM decide. External knowledge is injected only at step 2, so no gold labels are needed.</td>
            <td>Generates *silver* entity spans and types using only raw text and a UMLS dump—ideal for working with unannotated ultrasound reports.</td>
          </tr>
          <tr>
            <td><strong>GLiNER-biomed (Zero-shot)</strong></td>
            <td>A BERT-like encoder trained to recognize any entity list you provide. At inference, you pass a list of types (e.g., `["Observation", "Anatomy"]`) and GLiNER returns all spans it believes belong to those types, with no further fine-tuning.</td>
            <td>Instant, domain-agnostic span detection that can be bootstrapped with entity lists specific to ultrasound (e.g., liver terms, anatomical regions).</td>
          </tr>
          <tr>
            <td><strong>NegBio</strong></td>
            <td>Uses dependency-pattern rules on universal dependencies to label each candidate finding as *Negated*, *Uncertain*, or *Affirmed*. It outperforms classic NegEx by ~10 F1 points on radiology corpora.</td>
            <td>Converts spans from the first two methods into the *Exist* value of triplets, without needing any training.</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subtitle is-4">Related Approaches</h3>
      <table class="table is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Approach</th>
            <th>Implementation</th>
            <th>Pros / Cons</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Dictionary Projection</strong></td>
            <td>Map each candidate span to the nearest RadLex / ICD-10 / SNOMED term by cosine similarity in the `USLM` embedding space (threshold 0.75).</td>
            <td>Fast and explainable but struggles with novel jargon.</td>
          </tr>
          <tr>
            <td><strong>Zero-/Few-shot NER Teacher</strong></td>
            <td>Run GLiNER-biomed or BioGPT-NER in open mode with prompts like “Mark every observation and every anatomical site”. Keep spans whose confidence ≥ 0.9 as silver labels.</td>
            <td>Leverages LLM prior knowledge, increasing coverage, but may introduce noise.</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subtitle is-4">Unsupervised Entity Discovery</h3>
      <p>The goal of entity discovery is to carve the raw text into candidate span boundaries without assigning types yet. Here's a technique for **entity discovery**:</p>
      <ul>
        <li><strong>AutoPhrase / SegPhrase:</strong> High-precision phrase mining driven by PMI (Pointwise Mutual Information) and token statistics.</li>
        <li><strong>Embedding & Clustering:</strong> Embed every noun-phrase in the corpus using the <code>USLM</code> embedding model, then reduce dimensions with UMAP and cluster with HDBSCAN. Each cluster represents one concept family, like “hypoechoic nodule” or “right hepatic lobe”.</li>
        <li><strong>Filtering:</strong> Keep clusters whose head words occur in at least 30 documents (or a predefined support threshold).</li>
      </ul>

      <h4 class="subtitle is-5">Example Proto-Entities</h4>
      <ul>
        <li>{hypoechoic nodule, hypoechoic area}</li>
        <li>{right lobe, right hepatic lobe}</li>
        <li>{bile duct dilatation}</li>
      </ul>

      <p>These techniques enable automatic identification of potential entities in ultrasound reports, providing a foundation for downstream tasks like triplet extraction and model fine-tuning.</p>

      <h3 class="subtitle is-4">Key References</h3>
      <ul>
        <li>Bian et al., <em>Unsupervised BioNER Framework</em>, arXiv 2023.</li>
        <li>GLiNER-biomed Zero-shot Tagging, <a href="https://github.com/urchade/GLiNER/blob/main/README_Extended.md?utm_source=chatgpt.com" target="_blank">GitHub</a></li>
        <li>NegBio Negation/Uncertainty Rules, <em>RadGraph</em></li>
      </ul>
    </div>
  </div>
</section>

          <section>
    <h2>4. Unsupervised Evaluation of Models for Ultrasound Entity Recognition</h2>
    <p>Unsupervised evaluation techniques are essential when ground truth data is limited or unavailable. These methods focus on evaluating the consistency, coherence, and reliability of annotations across various reports. Below, we outline some effective unsupervised evaluation approaches for Named Entity Recognition (NER) in ultrasound reports.</p>

    <h3>Evaluation Approaches</h3>
    <table border="1" cellpadding="5">
        <thead>
            <tr>
                <th>Evaluation Method</th>
                <th>Description</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Entity Consistency Check</strong></td>
                <td>Measures how consistently the model extracts the same entities across similar reports. If the same entity is extracted with varying labels (e.g., "Cyst" vs. "Mass"), it signals inconsistency in the model's recognition.</td>
                <td>Useful for checking if terms like "Tumor" are always labeled consistently in different reports.</td>
            </tr>
            <tr>
                <td><strong>Clustering-Based Evaluation</strong></td>
                <td>Clustering extracted entities to check if similar entities are grouped together. For instance, "Liver Cyst," "Hepatic Cyst," and "Cyst in Liver" should be clustered together.</td>
                <td>To assess whether similar medical entities are classified together, ensuring accuracy in extracting anatomical findings.</td>
            </tr>
            <tr>
                <td><strong>Entity Relationship Consistency</strong></td>
                <td>Evaluates if entities are correctly related to anatomical positions. For example, "Cyst" should consistently be linked to "Liver" when described as a cyst in the liver.</td>
                <td>To verify that anatomical locations are correctly matched with associated entities (e.g., "Right Kidney" with "Cyst").</td>
            </tr>
            <tr>
                <td><strong>Named Entity Linking (NEL) Accuracy</strong></td>
                <td>Measures the model's ability to link extracted entities to a knowledge base such as UMLS or SNOMED CT. This ensures that the correct medical terms are used consistently.</td>
                <td>Essential for checking the correctness of terms like "Cyst" by matching them with standard medical databases.</td>
            </tr>
            <tr>
                <td><strong>Similarity Metrics</strong></td>
                <td>Uses semantic similarity metrics (e.g., Cosine Similarity, Jaccard Index) to assess whether similar terms are extracted consistently. For example, checking if "Liver" and "Hepatic" are identified as referring to the same anatomical region.</td>
                <td>Helps evaluate the semantic accuracy of entity extraction by ensuring that synonymous terms are recognized as identical concepts.</td>
            </tr>
        </tbody>
    </table>

    <p>These unsupervised evaluation techniques help identify issues in entity extraction consistency and ensure that the model is performing reliably across a wide range of ultrasound reports.</p>

    <h2>5. Triplet Extraction Using LLMs (e.g., GPT-4o) for Ultrasound Data Labeling</h2>
    <p>Using Large Language Models (LLMs) like GPT-4o for triplet extraction in ultrasound reports involves identifying key entities, anatomical locations, and their existence status (e.g., Present, Absent, Uncertain). The feasibility of this approach depends on fine-tuning LLMs on ultrasound-specific data and using a well-defined framework for triplet extraction.</p>

    <h3>Feasibility of Using LLMs for Ultrasound Data Labeling</h3>
    <p>LLMs like GPT-4o can be highly effective for ultrasound data labeling due to their foundational knowledge of medical texts, which includes anatomy, diseases, and medical terminology. However, to ensure accuracy, these models need to be fine-tuned on ultrasound-specific datasets, which helps the model learn the nuanced language of ultrasound reports.</p>

    <h4>Key Considerations:</h4>
    <ul>
        <li><strong>High Accuracy:</strong> LLMs can be trained to achieve high accuracy, but fine-tuning with ultrasound-specific reports is essential.</li>
        <li><strong>Cost-Effectiveness:</strong> Using LLMs for annotation reduces the need for manual expert annotation, which can be expensive.</li>
        <li><strong>Speed:</strong> LLMs can process large datasets rapidly, making them ideal for automating the annotation of ultrasound data.</li>
    </ul>

    <h3>How to Ensure LLM Does a Fine Job Labeling Ultrasound Data</h3>
    <p>To ensure that LLMs like GPT-4o accurately label ultrasound reports, the following strategies should be applied:</p>

    <h4>a. Fine-Tuning the Model</h4>
    <p>Fine-tuning the LLM on ultrasound-specific datasets is crucial. This allows the model to recognize medical terms and contextual nuances specific to ultrasound reports (e.g., “lesion in the left kidney”). Fine-tuning datasets should include pre-annotated ultrasound reports in the format of {Entity, Location, Existence}.</p>

    <h4>b. Using Few-Shot Prompting</h4>
    <p>In cases where labeled data is limited, few-shot learning can be applied by providing the LLM with a few examples of ultrasound reports with their corresponding annotations. This can help the model generalize to new data.</p>

    <h4>c. Incorporating Medical Knowledge Bases</h4>
    <p>Integrating external knowledge bases like UMLS or SNOMED CT into the LLM helps ensure that extracted entities and locations are consistent with standardized medical terms.</p>

    <h4>d. Confidence Scoring</h4>
    <p>LLMs can provide confidence scores for their predictions. Low-confidence predictions should be flagged for human review. For instance, the model might output the following:</p>
    <ul>
        <li>{Entity: Cyst, Position: Left kidney, Exist: Present, Confidence: 95%}</li>
        <li>{Entity: Tumor, Position: Liver, Exist: Uncertain, Confidence: 65%}</li>
    </ul>

    <h3>Triplet Extraction Example</h3>
    <p>The LLM can be prompted with the following structure to extract the triplets from an ultrasound report:</p>
    <pre>
    Prompt: “Extract the medical entity, its anatomical location, and its existence from the following ultrasound report:
    ‘A small tumor was found in the left liver lobe, but its presence is uncertain.’”
    </pre>
    <p>Expected Output: 
    <ul>
        <li>{Entity: Tumor, Location: Left liver lobe, Existence: Uncertain}</li>
    </ul></p>

    <p>By iterating on this process and adjusting the model’s prompts and training data, the LLM can accurately and consistently extract triplets from ultrasound reports.</p>
</section>


  <!-- ===== PLOTS SECTION ===== -->
  <section class="section has-background-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Model Evolution – Size × Performance</h2>
      <div class="columns is-variable is-8">
        <div class="column">
          <figure class="image is-3by2">
            <img src="static/images/plot_qa.png" alt="Bubble plot of PubMedQA accuracy over publication year" />
          </figure>
          <p class="has-text-centered is-size-6 mt-2"><em>Figure 1</em> – PubMedQA accuracy rises with newer &amp; larger models.</p>
        </div>
        <div class="column">
          <figure class="image is-3by2">
            <img src="static/images/plot_ner.png" alt="Bubble plot of BC5-disease F1 over publication year" />
          </figure>
          <p class="has-text-centered is-size-6 mt-2"><em>Figure 2</em> – Disease-NER performance has plateaued since 2019.</p>
        </div>
      </div>
    </div>
  </section>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


  <!-- ===== REFERENCES SECTION ===== -->
  <section class="section has-background-light">
    <div class="container is-max-desktop">
      <h2 class="title is-4">Key References</h2>
      <div class="content is-size-6">
        <ul>
          <li>Lee J. et al. <em>BioBERT: a pre-trained biomedical language representation model</em>. ACL 2019.</li>
          <li>Luo R. et al. <em>BioGPT: generative pre-trained transformer for biomedical text generation</em>. bioRxiv 2022.</li>
          <li>Nori H. et al. <em>Toward trustworthy DPO for medical report generation</em>. NAACL 2025.</li>
        </ul>
      </div>
    </div>
  </section>







  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>









