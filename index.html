<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ultrasound Report Generation DPO </title>
    <!-- MathJax for LaTeX rendering -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Ultrasound Report Generation DPO </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Nikkhah, Ali</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Heidari, Moein</a><sup>*</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>














<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
  <!-- ===== HERO ===== -->
  <section class="hero is-primary is-bold">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h1 class="title is-2">Ultrasound Report Generation with DPO</h1>
      </div>
    </div>
  </section>
      <section>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Flow</h2>
    <div class="content">
      <h3 class="title is-4">Implications for Triplet Extraction</h3>
      <p>A strong NER stage is crucial since any errors in entity recognition or hallucinated entities will propagate to the triplet extraction stage. Using a BioBERT-style model as the backbone, especially fine-tuned on ultrasound reports, will result in cleaner <code>{entity, position, exist}</code> triples.</p>

      <h4 class="subtitle is-5">Negation and Uncertainty Detection</h4>
      <p>Tools like <strong>NegBio</strong> (which employs dependency-pattern rules) can slot in after NER to determine the "existence" flag, improving the triplet consistency.</p>

      <h4 class="subtitle is-5">Position Phrase Parsing</h4>
      <p>Ultrasound-specific positions (e.g., "sub-hepatic region", "right adnexa") require curated lexicons or relation parsers like OSCAR/NegBio to map phrases to standard anatomical locations.</p>



      <h3 class="title is-4">End-to-End NER + Triplet Flow</h3>
      <p>Below is the revised pipeline for processing ultrasound reports:</p>
      <ol>
        <li><strong>Pre-process & Tokenize:</strong> Expand abbreviations, keep measurement tokens.</li>
        <li><strong>NER with BioBERT-family:</strong> Fine-tune BioBERT on manually-annotated ultrasound sentences for entity recognition.</li>
        <li><strong>Negation Detection:</strong> Use NegBio (or similar rule-based methods) to set the "exist" flag (True/False/Unknown).</li>
        <li><strong>Position Parsing:</strong> Use RadGraph or rule-based models for anatomical phrase matching (e.g., "left ovary").</li>
        <li><strong>Triplet Assembly:</strong> Assemble <code>{entity, position, exist}</code> triples.</li>
      </ol>

      <h3 class="title is-4">Entity Translation & MedKLIP Integration</h3>
      <p>Incorporating an <strong>Entity Translation</strong> layer, inspired by MedKLIP’s approach, will enhance the robustness and generalization of the NER system. Here’s how:</p>

      <ol>
        <li><strong>Canonical Concept Lookup:</strong> Map each raw entity to a canonical concept (e.g., "cyst" → UMLS CUI) using internal lookups or APIs.</li>
        <li><strong>Definition Sentence Retrieval:</strong> Use knowledge sources (e.g., Wikipedia, Radiopaedia) to fetch definitions for each entity.</li>
        <li><strong>Text-Encode Definitions:</strong> Encode these definitions with a model like ClinicalBERT to obtain embeddings, which carry richer semantic meaning.</li>
        <li><strong>Cache Vectors:</strong> Cache the resulting entity vectors for speed and continual updates.</li>
      </ol>

      <h3 class="title is-4">Final Triplet Assembly</h3>
      <p>The final output format of each triplet includes:</p>
      <pre>
        {
          "entity": {
            "text": "cyst",
            "cui": "C0010692",
            "embedding": [ … ]
          },
          "position": {
            "label": "left ovary",
            "prompt_embedding": [ … ]
          },
          "exist": "TRUE"
        }
      </pre>


  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Manual Annotation</h2>
    <div class="content">  

      <ol>
        <li>Generate weak labels automatically using <code>pseudo_label.py</code>.</li>
        <li>Import the labels into Label Studio using the "Sequence Labeling" template and BIO format.</li>
        <li>Have a radiologist (or yourself) correct ~300 sentences to clean the model’s worst mistakes.</li>
        <li>Export the corrected data to spaCy or BIO format and fine-tune the NER model for one epoch.</li>
        <li>(Optional) Switch to Prodigy for active learning if you want a continuous feedback loop.</li>
      </ol>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Domain-Adaptive Pretraining</h2>
    <div class="content">
      <p>Large-scale NLP systems like BERT use domain-adaptive pretraining (DAPT) to better capture the language of specific domains like ultrasound reports. Here’s the process:</p>
      
      <h3 class="subtitle is-4">DAPT / MLM (Masked Language Modeling)</h3>
      <p>Pre-train BERT using raw ultrasound report text, teaching the model the language of ultrasound (e.g., abbreviations, anatomical terms, measurement patterns) without requiring labels.</p>
      <p><strong>Inputs:</strong> Raw report text.</p>
      <p><strong>Cost (time/VRAM):</strong> Light. With LoRA and 8-bit precision, a 3070 4GB GPU can complete 3 epochs over ~50k sentences in under 1 hour.</p>
    </div>
  </div>
</section>
        <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Core Ontology and Medical Databases</h2>
        <table class="table is-hoverable is-fullwidth" border="1" cellpadding="5">
            <thead>
                <tr>
                    <th>Tier</th>
                    <th>Resource / API</th>
                    <th>What it gives you (Entity, Location, Synonyms)</th>
                    <th>How to integrate</th>
                    <th>Notes & Links</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Core Ontology</strong></td>
                    <td><strong>UMLS Metathesaurus</strong></td>
                    <td>Over 4 million CUIs covering disorders, findings, anatomy, devices, procedures. Synonym lists included.</td>
                    <td>1. Download yearly release <br> 2. Filter by semantic types (e.g., T033 – “Finding”, T023 – “Body part”) <br> 3. Generate JSON gazetteer for entity auto-completion.</td>
                    <td>“One-stop shop” for comprehensive medical terms. Filter aggressively. <br> <a href="https://www.nlm.nih.gov/research/umls/Snomed/SNOMED_CT_User_Guide_20080731.pdf?utm_source=chatgpt.com">SNOMED CT User Guide</a></td>
                </tr>
                <tr>
                    <td></td>
                    <td><strong>SNOMED CT</strong></td>
                    <td>Gold-standard hierarchy for clinical findings with explicit anatomical site relations.</td>
                    <td>Normalize location phrases like "RUQ", "right liver lobe", "segment VI" using the same concept ID.</td>
                    <td>Public release via NLM license. <a href="https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon?utm_source=chatgpt.com">RadLex</a></td>
                </tr>
                <tr>
                    <td></td>
                    <td><strong>RadLex</strong></td>
                    <td>Imaging-specific lexicon (lesions, artifacts, views, ultrasound probes, planes).</td>
                    <td>Append to UMLS for disambiguating imaging jargon.</td>
                    <td>Free from RSNA. <a href="https://www.rsna.org/practice-tools/data-tools-and-standards/radlex-radiology-lexicon?utm_source=chatgpt.com">RadLex radiology lexicon</a></td>
                </tr>
                <tr>
                    <td><strong>Light-weight, use-from-Python</strong></td>
                    <td><strong>QuickUMLS / medSpaCy-QuickUMLS</strong></td>
                    <td>Fast, fuzzy string-to-UMLS matcher returning [CUI, term, similarity].</td>
                    <td>Use as post-processor to match LLM proposals to UMLS terms with similarity above a set threshold.</td>
                    <td>Pip-installable, no license hurdles. <a href="https://github.com/Georgetown-IR-Lab/QuickUMLS?utm_source=chatgpt.com">QuickUMLS GitHub</a></td>
                </tr>
                <tr>
                    <td><strong>Structured location schema (ultrasound-friendly)</strong></td>
                    <td><strong>FMA (Foundational Model of Anatomy)</strong></td>
                    <td>Parent/child relations (e.g., "liver → lobe → segment"). Ideal for hierarchical location labels.</td>
                    <td>Use a three-level subset (organ, lobe/quadrant, sub-region) for location labels.</td>
                    <td></td>
                </tr>
                <tr>
                    <td><strong>Specialty Add-ons</strong></td>
                    <td><strong>HPO (phenotypes for obstetric US), LOINC (panels for vascular flow studies)</strong></td>
                    <td>Use if covering subspecialties like obstetric ultrasound or vascular studies.</td>
                    <td></td>
                    <td></td>
                </tr>
            </tbody>
        </table>
          <h2 class="title is-3 has-text-centered">Specialized Resources for Ultrasound Report Annotation</h2>
    <div class="content">
   <table class="table is-hoverable is-fullwidth" border="1" cellpadding="5">
        <thead>
          <tr>
            <th>Vocabulary</th>
            <th>File(s) You Need</th>
            <th>Direct Download Link</th>
            <th>License Note</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>RadLex 4.2</strong> (radiology observations + anatomy)</td>
            <td>RadLex_OWL4.2.zip (contains radlex.owl)</td>
            <td><a href="https://radlex.org/download/RadLex_OWL4.2.zip" target="_blank">RadLex 4.2 Download</a></td>
            <td>Free for research (RSNA license prompt)</td>
          </tr>
          <tr>
            <td><strong>ICD-10-CM FY-2025</strong> (disease codes)</td>
            <td>Code-descriptions-April-2025.zip – CSV of ICD-10-CM codes & text <br> icd10cm-table-index-April-2025.zip – Alphabetical index</td>
            <td><a href="https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD10CM/2025-Update/Code-desciptions-April-2025.zip" target="_blank">ICD-10-CM FY 2025 Codes</a> <br> <a href="https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD10CM/2025-Update/icd10cm-table-index-April-2025.zip" target="_blank">ICD-10-CM Index</a></td>
            <td>Public-domain</td>
          </tr>
          <tr>
            <td><strong>SNOMED CT INT 2024-07</strong> (findings, qualifiers)</td>
            <td>SnomedCT_InternationalRF2_production.zip</td>
            <td><a href="https://www.nlm.nih.gov/research/umls/Snomed/SNOMED_CT_User_Guide_20080731.pdf" target="_blank">SNOMED CT Overview</a></td>
            <td>Non-commercial research allowed</td>
          </tr>
          <tr>
            <td><strong>MIDO</strong> (medical-imaging ontology, ultrasound terms)</td>
            <td>mido.owl via BioPortal</td>
            <td><a href="https://bioportal.bioontology.org/ontologies/MIDO?download_file=true" target="_blank">MIDO Download</a></td>
            <td>CC-BY</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subtitle is-4">Other Resources</h3>
      <ul>
        <li><strong>BI-RADS (Breast Imaging-Reporting and Data System):</strong> A standardized system for reporting breast imaging findings, including those from ultrasound. Available from the American College of Radiology.</li>
        <li><strong>LI-RADS (Liver Imaging Reporting and Data System):</strong> A system for classifying liver lesions based on imaging features, including those observed in ultrasound. Guidelines are publicly available from the American College of Radiology.</li>
        <li><strong>Medical Imaging and Diagnostic Ontology (MIDO):</strong> Available on BioPortal, designed to standardize data in medical imaging tasks, including disease classification and imaging modalities.</li>
      </ul>

      <h3 class="subtitle is-4">Tools for Integrating These Resources</h3>
      <ul>
        <li><a href="https://github.com/med-spacy/medspaCy" target="_blank"><strong>medSpaCy</strong></a>: A SpaCy extension for clinical text processing, including pre-trained models for NER in medical contexts.</li>
        <li><a href="https://github.com/allenai/scispacy" target="_blank"><strong>scispaCy</strong></a>: A SpaCy-based library providing pre-trained models for scientific and clinical text processing.</li>
        <li><a href="https://www.johnsnowlabs.com/spark-nlp/" target="_blank"><strong>Spark NLP for Healthcare</strong></a>: A library offering pre-trained models for healthcare NLP tasks, including NER and clinical entity resolution.</li>
      </ul>
    </div>
    </section>



  <!-- ===== MODEL TABLE SECTION ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Top Performing Open-Source Biomedical LMs</h2>
    <div class="content">
   <table class="table is-hoverable is-fullwidth" border="1" cellpadding="5">
        <thead>
          <tr>
            <th>Model</th>
            <th>Training Corpus</th>
            <th>Open-Source?</th>
            <th>Params (M)</th>
            <th>PubMedQA Acc. (%)</th>
            <th>BC5-Disease F1 (%)</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>BioBERT</td><td>PubMed + PMC abstracts</td><td class="has-text-success">✔</td><td>110</td><td>60.2</td><td>84.7</td></tr>
          <tr><td>SciBERT</td><td>Semantic Scholar</td><td class="has-text-success">✔</td><td>110</td><td>57.4</td><td>84.5</td></tr>
          <tr><td>BlueBERT</td><td>PubMed + MIMIC-III</td><td class="has-text-success">✔</td><td>110</td><td>49.1</td><td>83.0</td></tr>
          <tr><td>PubMedBERT</td><td>PubMed abstracts/full-text</td><td class="has-text-success">✔</td><td>110</td><td>55.8</td><td>85.6</td></tr>
          <tr><td>BioGPT-Large</td><td>PubMed abstracts</td><td class="has-text-success">✔</td><td>1500</td><td><strong>78.2</strong></td><td>45.0†</td></tr>
          <tr><td>BioMedLM</td><td>PubMed + PMC full-text</td><td class="has-text-success">✔</td><td>2700</td><td>74.4</td><td>—</td></tr>
          <tr><td>PMC LLaMA-13B</td><td>PMC full-text</td><td class="has-text-success">✔</td><td>13000</td><td>76.8</td><td>—</td></tr>
          <tr><td><strong>BioMegatron-345M</strong></td><td>PubMed + clinical notes</td><td class="has-text-success">✔</td><td>345</td><td>—</td><td>—</td></tr>
          <tr><td><strong>BioMed-RoBERTa-large (355M)</strong></td><td>Biomedical corpus (PubMed, etc.)</td><td class="has-text-success">✔</td><td>355</td><td>—</td><td>—</td></tr>
          <tr><td><strong>BioGPT-Large (1.5B)</strong></td><td>PubMed abstracts</td><td class="has-text-success">✔</td><td>1500</td><td>—</td><td>—</td></tr>
          <tr><td><strong>Llama-2-7B (general-domain)</strong></td><td>General domain + PubMed continued pretrain</td><td class="has-text-success">✔</td><td>7000</td><td>—</td><td>—</td></tr>
        </tbody>
      </table>
      <p class="is-size-7">† BioGPT numbers are zero-shot; all others are fine-tuned on BLURB.</p>
    </div>
  </div>
</section>
  <!-- ===== PLOTS SECTION ===== -->
  <section class="section has-background-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Model Evolution – Size × Performance</h2>
      <div class="columns is-variable is-8">
        <div class="column">
          <figure class="image is-3by2">
            <img src="static/images/plot_qa.png" alt="Bubble plot of PubMedQA accuracy over publication year" />
          </figure>
          <p class="has-text-centered is-size-6 mt-2"><em>Figure 1</em> – PubMedQA accuracy rises with newer &amp; larger models.</p>
        </div>
        <div class="column">
          <figure class="image is-3by2">
            <img src="static/images/plot_ner.png" alt="Bubble plot of BC5-disease F1 over publication year" />
          </figure>
          <p class="has-text-centered is-size-6 mt-2"><em>Figure 2</em> – Disease-NER performance has plateaued since 2019.</p>
        </div>
      </div>
    </div>
  </section>
  <section class="section">

    <h2 class="title is-3 has-text-centered">Unsupervised Learning for Annotation</h2>
    <div class="content">
      <p>Unsupervised learning methods are crucial for handling *untagged* ultrasound reports where no labeled data is available. These techniques allow you to detect entities, their types, and handle negation without requiring manual annotations. Here are the most relevant methods:</p>

   <table class="table is-hoverable is-fullwidth" border="1" cellpadding="5">
        <thead>
          <tr>
            <th>Method</th>
            <th>Core Idea</th>
            <th>Why It Matters for <em>Untagged</em> Ultrasound Reports</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Unsupervised BioNER Framework (Bian et al., 2023)</strong></td>
            <td>Treats NER as two unsupervised steps: (1) entity-span discovery using LLM prompts, (2) entity-type assignment by retrieving similar concepts from UMLS and letting the LLM decide. External knowledge is injected only at step 2, so no gold labels are needed.</td>
            <td>Generates *silver* entity spans and types using only raw text and a UMLS dump—ideal for working with unannotated ultrasound reports.</td>
          </tr>
          <tr>
            <td><strong>GLiNER-biomed (Zero-shot)</strong></td>
            <td>A BERT-like encoder trained to recognize any entity list you provide. At inference, you pass a list of types (e.g., `["Observation", "Anatomy"]`) and GLiNER returns all spans it believes belong to those types, with no further fine-tuning.</td>
            <td>Instant, domain-agnostic span detection that can be bootstrapped with entity lists specific to ultrasound (e.g., liver terms, anatomical regions).</td>
          </tr>
          <tr>
            <td><strong>NegBio</strong></td>
            <td>Uses dependency-pattern rules on universal dependencies to label each candidate finding as *Negated*, *Uncertain*, or *Affirmed*. It outperforms classic NegEx by ~10 F1 points on radiology corpora.</td>
            <td>Converts spans from the first two methods into the *Exist* value of triplets, without needing any training.</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subtitle is-4">Related Approaches</h3>
   <table class="table is-hoverable is-fullwidth" border="1" cellpadding="5">
        <thead>
          <tr>
            <th>Approach</th>
            <th>Implementation</th>
            <th>Pros / Cons</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Dictionary Projection</strong></td>
            <td>Map each candidate span to the nearest RadLex / ICD-10 / SNOMED term by cosine similarity in the `USLM` embedding space (threshold 0.75).</td>
            <td>Fast and explainable but struggles with novel jargon.</td>
          </tr>
          <tr>
            <td><strong>Zero-/Few-shot NER Teacher</strong></td>
            <td>Run GLiNER-biomed or BioGPT-NER in open mode with prompts like “Mark every observation and every anatomical site”. Keep spans whose confidence ≥ 0.9 as silver labels.</td>
            <td>Leverages LLM prior knowledge, increasing coverage, but may introduce noise.</td>
          </tr>
        </tbody>
      </table>

      <h3 class="subtitle is-4">Unsupervised Entity Discovery</h3>
      <p>The goal of entity discovery is to carve the raw text into candidate span boundaries without assigning types yet. Here's a technique for **entity discovery**:</p>
      <ul>
        <li><strong>AutoPhrase / SegPhrase:</strong> High-precision phrase mining driven by PMI (Pointwise Mutual Information) and token statistics.</li>
        <li><strong>Embedding & Clustering:</strong> Embed every noun-phrase in the corpus using the <code>USLM</code> embedding model, then reduce dimensions with UMAP and cluster with HDBSCAN. Each cluster represents one concept family, like “hypoechoic nodule” or “right hepatic lobe”.</li>
        <li><strong>Filtering:</strong> Keep clusters whose head words occur in at least 30 documents (or a predefined support threshold).</li>
      </ul>


      <h3 class="subtitle is-4">Key References</h3>
      <ul>
        <li>Bian et al., <em>Unsupervised BioNER Framework</em>, arXiv 2023.</li>
        <li>GLiNER-biomed Zero-shot Tagging, <a href="https://github.com/urchade/GLiNER/blob/main/README_Extended.md?utm_source=chatgpt.com" target="_blank">GitHub</a></li>
        <li>NegBio Negation/Uncertainty Rules, <em>RadGraph</em></li>
      </ul>


            
    <h3>Unsupervised Evaluation</h3>
    <p>Unsupervised evaluation techniques are essential when ground truth data is limited or unavailable. These methods focus on evaluating the consistency, coherence, and reliability of annotations across various reports. Below, we outline some effective unsupervised evaluation approaches for Named Entity Recognition (NER) in ultrasound reports.</p>

    <h3>Evaluation Approaches</h3>
    <table class="table is-hoverable is-fullwidth" border="1" cellpadding="5">
        <thead>
            <tr>
                <th>Evaluation Method</th>
                <th>Description</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Entity Consistency Check</strong></td>
                <td>Measures how consistently the model extracts the same entities across similar reports. If the same entity is extracted with varying labels (e.g., "Cyst" vs. "Mass"), it signals inconsistency in the model's recognition.</td>
                <td>Useful for checking if terms like "Tumor" are always labeled consistently in different reports.</td>
            </tr>
            <tr>
                <td><strong>Clustering-Based Evaluation</strong></td>
                <td>Clustering extracted entities to check if similar entities are grouped together. For instance, "Liver Cyst," "Hepatic Cyst," and "Cyst in Liver" should be clustered together.</td>
                <td>To assess whether similar medical entities are classified together, ensuring accuracy in extracting anatomical findings.</td>
            </tr>
            <tr>
                <td><strong>Entity Relationship Consistency</strong></td>
                <td>Evaluates if entities are correctly related to anatomical positions. For example, "Cyst" should consistently be linked to "Liver" when described as a cyst in the liver.</td>
                <td>To verify that anatomical locations are correctly matched with associated entities (e.g., "Right Kidney" with "Cyst").</td>
            </tr>
            <tr>
                <td><strong>Named Entity Linking (NEL) Accuracy</strong></td>
                <td>Measures the model's ability to link extracted entities to a knowledge base such as UMLS or SNOMED CT. This ensures that the correct medical terms are used consistently.</td>
                <td>Essential for checking the correctness of terms like "Cyst" by matching them with standard medical databases.</td>
            </tr>
            <tr>
                <td><strong>Similarity Metrics</strong></td>
                <td>Uses semantic similarity metrics (e.g., Cosine Similarity, Jaccard Index) to assess whether similar terms are extracted consistently. For example, checking if "Liver" and "Hepatic" are identified as referring to the same anatomical region.</td>
                <td>Helps evaluate the semantic accuracy of entity extraction by ensuring that synonymous terms are recognized as identical concepts.</td>
            </tr>
        </tbody>
    </table>

</section>

<section>
    <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Triplet Extraction Using LLMs (e.g., GPT-4o) for Ultrasound Data Labeling</h2>

    <p>Using Large Language Models (LLMs) like GPT-4o for triplet extraction in ultrasound reports involves identifying key entities, anatomical locations, and their existence status (e.g., Present, Absent, Uncertain). The feasibility of this approach depends on fine-tuning LLMs on ultrasound-specific data and using a well-defined framework for triplet extraction.</p>

    <h3>Feasibility of Using LLMs for Ultrasound Data Labeling</h3>
    <p>LLMs like GPT-4o can be highly effective for ultrasound data labeling due to their foundational knowledge of medical texts, which includes anatomy, diseases, and medical terminology. However, to ensure accuracy, these models need to be fine-tuned on ultrasound-specific datasets, which helps the model learn the nuanced language of ultrasound reports.</p>

    <h4>Key Considerations:</h4>
    <ul>
        <li><strong>High Accuracy:</strong> LLMs can be trained to achieve high accuracy, but fine-tuning with ultrasound-specific reports is essential.</li>
        <li><strong>Cost-Effectiveness:</strong> Using LLMs for annotation reduces the need for manual expert annotation, which can be expensive.</li>
        <li><strong>Speed:</strong> LLMs can process large datasets rapidly, making them ideal for automating the annotation of ultrasound data.</li>
    </ul>

    <h3>How to Ensure LLM Does a Fine Job Labeling Ultrasound Data</h3>
    <p>To ensure that LLMs like GPT-4o accurately label ultrasound reports, the following strategies should be applied:</p>

    <h4>a. Fine-Tuning the Model</h4>
    <p>Fine-tuning the LLM on ultrasound-specific datasets is crucial. This allows the model to recognize medical terms and contextual nuances specific to ultrasound reports (e.g., “lesion in the left kidney”). Fine-tuning datasets should include pre-annotated ultrasound reports in the format of {Entity, Location, Existence}.</p>

    <h4>b. Using Few-Shot Prompting</h4>
    <p>In cases where labeled data is limited, few-shot learning can be applied by providing the LLM with a few examples of ultrasound reports with their corresponding annotations. This can help the model generalize to new data.</p>

    <h4>c. Incorporating Medical Knowledge Bases</h4>
    <p>Integrating external knowledge bases like UMLS or SNOMED CT into the LLM helps ensure that extracted entities and locations are consistent with standardized medical terms.</p>

    <h4>d. Confidence Scoring</h4>
    <p>LLMs can provide confidence scores for their predictions. Low-confidence predictions should be flagged for human review. For instance, the model might output the following:</p>
    <ul>
        <li>{Entity: Cyst, Position: Left kidney, Exist: Present, Confidence: 95%}</li>
        <li>{Entity: Tumor, Position: Liver, Exist: Uncertain, Confidence: 65%}</li>
    </ul>

    <h3>Triplet Extraction Example</h3>
    <p>The LLM can be prompted with the following structure to extract the triplets from an ultrasound report:</p>
    <pre>
    Prompt: “Extract the medical entity, its anatomical location, and its existence from the following ultrasound report:
    ‘A small tumor was found in the left liver lobe, but its presence is uncertain.’”
    </pre>
    <p>Expected Output: 
    <ul>
        <li>{Entity: Tumor, Location: Left liver lobe, Existence: Uncertain}</li>
    </ul></p>

    <p>By iterating on this process and adjusting the model’s prompts and training data, the LLM can accurately and consistently extract triplets from ultrasound reports.</p>
    </div>
</section>

        </div>
      </div>
    </div>
  </div>
</section>












<!-- End paper abstract -->


  <!-- ===== REFERENCES SECTION ===== -->
  <section class="section has-background-light">
    <div class="container is-max-desktop">
      <h2 class="title is-4">Key References</h2>
      <div class="content is-size-6">
        <ul>
          <li>Lee J. et al. <em>BioBERT: a pre-trained biomedical language representation model</em>. ACL 2019.</li>
          <li>Luo R. et al. <em>BioGPT: generative pre-trained transformer for biomedical text generation</em>. bioRxiv 2022.</li>
          <li>Nori H. et al. <em>Toward trustworthy DPO for medical report generation</em>. NAACL 2025.</li>
        </ul>
      </div>
    </div>
  </section>







  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
